{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define cv2_imshow from google collab because it wont install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython import display\n",
    "import PIL\n",
    "\n",
    "def cv2_imshow(a):\n",
    "  \"\"\"A replacement for cv2.imshow() for use in Jupyter notebooks.\n",
    "\n",
    "  Args:\n",
    "    a : np.ndarray. shape (N, M) or (N, M, 1) is an NxM grayscale image. shape\n",
    "      (N, M, 3) is an NxM BGR color image. shape (N, M, 4) is an NxM BGRA color\n",
    "      image.\n",
    "  \"\"\"\n",
    "  a = a.clip(0, 255).astype('uint8')\n",
    "  # cv2 stores colors as BGR; convert to RGB\n",
    "  if a.ndim == 3:\n",
    "    if a.shape[2] == 4:\n",
    "      a = cv2.cvtColor(a, cv2.COLOR_BGRA2RGBA)\n",
    "    else:\n",
    "      a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n",
    "  display.display(PIL.Image.fromarray(a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure our Model\n",
    "\n",
    "using PyTorch download a pre-trained YOLOv5 model for image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athielking/src/person-recognizer/.venv/lib/python3.8/site-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /home/athielking/.cache/torch/hub/master.zip\n",
      "YOLOv5 ðŸš€ 2023-9-12 Python-3.8.10 torch-2.0.1+cpu CPU\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1M/14.1M [00:00<00:00, 45.9MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "model.conf = 0.5 #NMS Confidence Threshold\n",
    "model.multi_label = False\n",
    "model.classes = [0] # Detect people only\n",
    "model.max_det = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Images\n",
    "\n",
    "loop through our directory and see what images have people in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         xmin        ymin         xmax         ymax  confidence  class    name\n",
      "0  575.420166  450.216278  1242.025757  1598.919678    0.840980      0  person\n",
      "1   50.831242  105.015335   682.140137  1600.000000    0.696541      0  person\n",
      "          xmin         ymin         xmax         ymax  confidence  class  \\\n",
      "0   120.656799  2609.779785  1210.084961  6201.459961    0.663172      0   \n",
      "1  2370.112305  2907.695068  2869.566162  3828.691162    0.606550      0   \n",
      "2   447.991119   787.123779  2354.145508  6496.900391    0.523442      0   \n",
      "\n",
      "     name  \n",
      "0  person  \n",
      "1  person  \n",
      "2  person  \n",
      "          xmin         ymin         xmax         ymax  confidence  class  \\\n",
      "0  1178.602295   866.873657  3752.710938  5972.000000    0.898618      0   \n",
      "1  3722.139404  3340.661865  4478.321289  5940.788574    0.818310      0   \n",
      "\n",
      "     name  \n",
      "0  person  \n",
      "1  person  \n",
      "          xmin         ymin         xmax         ymax  confidence  class  \\\n",
      "0     4.422177  1773.255127   590.005005  4462.509277    0.916027      0   \n",
      "1  5098.242188     0.000000  6708.588379  4410.130859    0.887664      0   \n",
      "2  1862.165894   715.859863  3415.963135  4397.124512    0.840893      0   \n",
      "3  3222.521729  1731.960205  4136.589844  4449.536621    0.831082      0   \n",
      "4  1488.623779  2427.260254  1912.954102  3887.582031    0.783647      0   \n",
      "5  4174.258789  1914.994995  4903.221680  4319.349121    0.669674      0   \n",
      "6  3955.970215  2535.786621  4296.140625  3813.107666    0.595304      0   \n",
      "\n",
      "     name  \n",
      "0  person  \n",
      "1  person  \n",
      "2  person  \n",
      "3  person  \n",
      "4  person  \n",
      "5  person  \n",
      "6  person  \n",
      "          xmin         ymin         xmax         ymax  confidence  class  \\\n",
      "0   144.291611  1036.520386  1071.616577  4280.035645    0.895875      0   \n",
      "1  4581.039062   999.096802  5674.473145  4233.513672    0.894108      0   \n",
      "2  2559.264160   899.502441  3374.491211  3968.107666    0.887658      0   \n",
      "3   903.775818   863.141174  1848.477173  4212.102051    0.865514      0   \n",
      "4  3226.701416   834.644287  4169.028320  3960.620117    0.812542      0   \n",
      "5  5461.419922   967.859497  6394.418457  3991.435059    0.736949      0   \n",
      "\n",
      "     name  \n",
      "0  person  \n",
      "1  person  \n",
      "2  person  \n",
      "3  person  \n",
      "4  person  \n",
      "5  person  \n",
      "          xmin        ymin         xmax         ymax  confidence  class  \\\n",
      "0   870.217346  539.055542  2167.730469  4375.276855    0.848484      0   \n",
      "1  1821.081421  991.646301  4177.705566  5115.964844    0.807631      0   \n",
      "\n",
      "     name  \n",
      "0  person  \n",
      "1  person  \n",
      "          xmin         ymin         xmax         ymax  confidence  class  \\\n",
      "0   475.164795   393.727936  2543.688477  4449.780273    0.888692      0   \n",
      "1  2872.358887  1777.573730  5277.412598  4480.000000    0.866636      0   \n",
      "\n",
      "     name  \n",
      "0  person  \n",
      "1  person  \n",
      "          xmin         ymin         xmax         ymax  confidence  class  \\\n",
      "0  3715.804688  1510.237915  4704.354492  4454.152344    0.869663      0   \n",
      "1  2367.787109   366.649109  4215.713379  4458.654297    0.808730      0   \n",
      "\n",
      "     name  \n",
      "0  person  \n",
      "1  person  \n",
      "          xmin         ymin        xmax         ymax  confidence  class  \\\n",
      "0  4060.250732  2153.622559  5045.46582  4398.392578    0.823773      0   \n",
      "\n",
      "     name  \n",
      "0  person  \n",
      "       xmin       ymin    xmax    ymax  confidence  class    name\n",
      "0  2.581694  26.588331  1137.0  1486.0    0.918819      0  person\n",
      "        xmin       ymin        xmax   ymax  confidence  class    name\n",
      "0  68.152557  22.500214  294.584412  320.0    0.765271      0  person\n",
      "      xmin      ymin        xmax        ymax  confidence  class    name\n",
      "0  44.4562  24.95937  277.244354  444.623199    0.896434      0  person\n",
      "Empty DataFrame\n",
      "Columns: [xmin, ymin, xmax, ymax, confidence, class, name]\n",
      "Index: []\n",
      "          xmin        ymin         xmax         ymax  confidence  class  \\\n",
      "0   350.899994  818.898499  1117.732178  1434.705933    0.883158      0   \n",
      "1   417.231018  163.924271   519.484558   259.159576    0.829313      0   \n",
      "2   911.154358   84.845795   959.013550   175.683472    0.725811      0   \n",
      "3   580.567688  166.988922   620.942566   230.761749    0.697739      0   \n",
      "4    54.575439  173.127701   143.389313   291.048370    0.652192      0   \n",
      "5  1404.924805  113.236038  1440.000000   173.707275    0.595491      0   \n",
      "6   522.969055   98.275620   546.239746   143.983398    0.574102      0   \n",
      "7  1350.902832  106.125008  1402.334229   178.902374    0.557470      0   \n",
      "8   547.721924  100.157730   572.475281   140.940765    0.511502      0   \n",
      "\n",
      "     name  \n",
      "0  person  \n",
      "1  person  \n",
      "2  person  \n",
      "3  person  \n",
      "4  person  \n",
      "5  person  \n",
      "6  person  \n",
      "7  person  \n",
      "8  person  \n",
      "         xmin        ymin        xmax        ymax  confidence  class    name\n",
      "0  563.657959  335.660370  651.882385  577.635315    0.798151      0  person\n",
      "1  933.436584  332.964264  960.723755  420.882568    0.780603      0  person\n",
      "         xmin        ymin         xmax         ymax  confidence  class    name\n",
      "0  877.635620   40.278385  1334.933716   735.766357    0.730951      0  person\n",
      "1    7.707029  190.622879   459.041412  1440.000000    0.593883      0  person\n",
      "          xmin        ymin         xmax        ymax  confidence  class    name\n",
      "0   658.660583  450.847656   922.306885  786.297913    0.940042      0  person\n",
      "1  1258.103760  469.442780  1375.900269  627.433105    0.899611      0  person\n",
      "2   699.552368  358.685669   753.593445  402.012024    0.707575      0  person\n",
      "Empty DataFrame\n",
      "Columns: [xmin, ymin, xmax, ymax, confidence, class, name]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [xmin, ymin, xmax, ymax, confidence, class, name]\n",
      "Index: []\n",
      "         xmin        ymin         xmax    ymax  confidence  class    name\n",
      "0  151.944077  136.166458  1102.444336  1536.0    0.788582      0  person\n",
      "         xmin        ymin        xmax         ymax  confidence  class    name\n",
      "0  396.369751  209.528732  903.387329  1264.342285     0.88879      0  person\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory_path = '/mnt/c/Users/athie/Pictures'\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\")):\n",
    "\n",
    "        image_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Open and process the image using OpenCV\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB format\n",
    "       \n",
    "        # Perform image processing or analysis here\n",
    "        with torch.no_grad():\n",
    "            results = model(img)\n",
    "            results.render()\n",
    "            dataFrame = results.pandas().xyxy[0]\n",
    "            print(dataFrame)\n",
    "            # cv2_imshow(results.ims[0])\n",
    "            # p = results.pandas().xyxy[0]\n",
    "            # for idx, row in p.iterrows():\n",
    "            #     print(\"PERSON FOUND\")\n",
    "            #     print(image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
